{
 "metadata": {
  "name": "",
  "signature": "sha256:b06c20de720c94c21f56193afea78bf7afd3eb93668fab4561625f9ec455790d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tutorial for `history_diagnostics`\n",
      "\n",
      "`history_diagnostics` is a pure-Python package to validate A/B test results based on contexts containing metrics relevant to the test. The validation of the test variants is performed by comparing the contexts to the values for a pre-experiment sample. This helps to increase the reliability of the validation. The code is available on [github](https://github.com/sevenval/history-diagnostics).\n",
      "\n",
      "In this tutorial, we will validate an A/B test with samples containing only a ~100 requests each."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from history_diagnostics import targetspace"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load example data\n",
      "\n",
      "First we need to get some data to work on. We use the example data, which is stored in a JSON file. Using `pandas`, we can easily generate `Sample` objects.\n",
      "\n",
      "`data/example.json` contains three data samples:\n",
      "\n",
      "*pre-experiment* contains a sample taken before the A/B test was started.\n",
      "\n",
      "*A* and *B* contain the samples for the two test cases. Since the application's traffic was split 50/50 on the two test-cases, we need to tell the sample that it has a `traffic_boost` given by `1 / traffic_share`. So, it's 2 for *A* and *B*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = json.load(open(\"data/example.json\"))\n",
      "s_pre = targetspace.Sample(pd.DataFrame(data[\"pre-experiment\"]))\n",
      "s_a = targetspace.Sample(pd.DataFrame(data[\"A\"]), traffic_boost=2)\n",
      "s_b = targetspace.Sample(pd.DataFrame(data[\"B\"]), traffic_boost=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Frontend context\n",
      "\n",
      "Next, we need to define our context function. In this tutorial, we focus on frontend-centric metrics. So the context consists of the following metrics:\n",
      "\n",
      "* traffic (optionally boosted for test-variant samples).\n",
      "* median performance\n",
      "* error rate normed to traffic\n",
      "* rate of some exemplary event \"#1\" also normed to traffic\n",
      "\n",
      "The function will be called with a single argument: a `Sample` object.\n",
      "\n",
      "We annotate the context function with the possible ranges of the metrics passed as `(low, high)` tuples. `None` means that no bound exists. There is no theoretical upper bound for the traffic rate or performance but the error and event rates are limitted to the interval `(0, 1)`.\n",
      "\n",
      "We also annotate the context function with the direction of the metrics: `\"upper\"` meaning that the metric gets better if it's increasing (traffic of event rate) and `\"lower\"` the opposite."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@targetspace.metric_directions(\"upper\", \"lower\", \"lower\", \"upper\")\n",
      "@targetspace.metric_ranges((0, None), (0, None), (0, 1), (0, 1))\n",
      "def frontend_context(sample):\n",
      "    n_requests = len(sample)\n",
      "    \n",
      "    if n_requests == 0:\n",
      "        return 0, 0, 0, 0\n",
      "    \n",
      "    traffic = n_requests / sample.duration * sample.traffic_boost\n",
      "    performance = np.median(sample.requests.performance)\n",
      "    error_rate = sample.requests.js_error.sum() / n_requests\n",
      "    event1_rate = sample.requests.event1.sum() / n_requests\n",
      "    \n",
      "    return traffic, performance, error_rate, event1_rate"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now peak into our data:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the traffic and event rate do not change so much. The performance of *B* improved by a factor of 2 compared to *pre-test* and *A* and the error rate worsened by a factor of 4.\n",
      "\n",
      "Let's see if can detect this."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Target space\n",
      "\n",
      "To perform the actual validation of the test, we need a target space. We need to create a sub-class of `TargetSpace` that inherits from a metric-combining mixin. In this case, we use the `OneMinusMaxMixin`, which returns the minimum of the probabilities to find a worse value for any of the metrics in the context function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FrontendTargetSpace(targetspace.OneMinusMaxMixin, targetspace.TargetSpace):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To create an instance, we must pass the context function and the pre-experiment sample. The pre-experiment sample can be referenced by the `history` attribute of the instance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ts = FrontendTargetSpace(frontend_context, s_pre)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we need to calibrate the target space. We have to do this for each of the test samples by passing\n",
      "* the ratio of the durations `s.duration / ts.history.duration`.\n",
      "* the number of bootstrap samples (We use 500, which is rather larger).\n",
      "* the time-window to group requests for bootstrap samling."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ts.calibrate(s_a.duration / ts.history.duration, 200, 0.1 * ts.history.duration / len(ts.history), True)\n",
      "ts.calibrate(s_b.duration / ts.history.duration, 200, 0.1 * ts.history.duration / len(ts.history), True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the calibration data is is cached. If you want/need to change the parameters of the bootstrap sampling, you have to pass `recalibrate=True` as additional parameter.\n",
      "\n",
      "## Validation\n",
      "\n",
      "Now we can validate the test by locating our test-samples in the target space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loc_a = ts.locate(s_a)\n",
      "loc_b = ts.locate(s_b)\n",
      "\n",
      "print(\"A: {:.5f}\".format(loc_a))\n",
      "print(\"B: {:.5f}\".format(loc_b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A: 0.17500\n",
        "B: 0.02000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that for variant *A* the smallest probability to find a worse metric value than observed is 0.15, so much above a critical value of 0.05. For variant *B*, this probability is only 0.035. This indicates that there might exist an unnoticed issue with this variant.\n",
      "\n",
      "Let's see, if we can find the culprit by comparing the metric values directly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metric_labels = (\"traffic\", \"performance\", \"error rate\", \"event rate\")\n",
      "metric_values = ((\"pre\", frontend_context(ts.history)),\n",
      "                 (\"A\", frontend_context(s_a)),\n",
      "                 (\"B\", frontend_context(s_b)))\n",
      "\n",
      "for i, metric_label in enumerate(metric_labels):\n",
      "    print(metric_label)\n",
      "    for sample_label, sample_values in metric_values:\n",
      "        print(\"  {}: {}\".format(sample_label, sample_values[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "traffic\n",
        "  pre: 0.00019892527693762726\n",
        "  A: 9.962747235310116e-05\n",
        "  B: 9.396507163311686e-05\n",
        "performance\n",
        "  pre: 2230.5\n",
        "  A: 2068.5\n",
        "  B: 1156.0\n",
        "error rate\n",
        "  pre: 0.01764705882352941\n",
        "  A: 0.0\n",
        "  B: 0.052083333333333336\n",
        "event rate\n",
        "  pre: 0.04411764705882353\n",
        "  A: 0.029411764705882353\n",
        "  B: 0.041666666666666664\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the error rate is the metric that worsened in variant *B* compared to the pre-experiment sample: 0.052 vs. 0.018 ."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}